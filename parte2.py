# %% [markdown]
# # Telecom X ‚Äì Parte 2: Predicci√≥n de Cancelaci√≥n (Churn)
#
# **Autor:** Analista Junior de Machine Learning
# **Misi√≥n:** Desarrollar un pipeline de Machine Learning para predecir la cancelaci√≥n de clientes, identificar los factores clave y proponer acciones estrat√©gicas.

# %% [markdown]
# ## 1Ô∏è‚É£ Configuraci√≥n del Entorno
# Importaci√≥n de las librer√≠as necesarias para el an√°lisis, preprocesamiento, modelado y evaluaci√≥n.

# %%
# Manipulaci√≥n y an√°lisis de datos
import pandas as pd
import numpy as np

# Visualizaciones
import matplotlib.pyplot as plt
import seaborn as sns

# Preprocesamiento de datos
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Modelos de clasificaci√≥n
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier

# M√©tricas de evaluaci√≥n
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve

# %% [markdown]
# ## 2Ô∏è‚É£ Carga y Preparaci√≥n de Datos
# Replicamos los pasos iniciales de la Parte 1 para cargar y aplanar los datos, asegurando un DataFrame limpio y listo para el preprocesamiento.

# %%
# Cargar el archivo JSON
df = pd.read_json('TelecomX_Data.json')

# Normalizar columnas anidadas (JSON)
customer_df = pd.json_normalize(df['customer'])
phone_df = pd.json_normalize(df['phone'])
internet_df = pd.json_normalize(df['internet'])
account_df = pd.json_normalize(df['account'])

# Concatenar en un √∫nico DataFrame plano
df_flat = pd.concat([
    df[['customerID']],
    customer_df,
    phone_df,
    internet_df,
    account_df,
    df['Churn']  # A√±adir la columna Churn al final
], axis=1)

# Limpieza inicial
# Eliminar filas donde 'Churn' es un string vac√≠o
df_flat = df_flat[df_flat['Churn'] != ""]

# Convertir 'Charges.Total' a num√©rico, los errores se convierten en NaN
df_flat['Charges.Total'] = pd.to_numeric(df_flat['Charges.Total'], errors='coerce')

print("Dimensiones del DataFrame:", df_flat.shape)
df_flat.head()

# %% [markdown]
# ## 3Ô∏è‚É£ Preprocesamiento de Datos para Machine Learning
# Esta etapa es crucial. Transformaremos los datos para que los algoritmos puedan interpretarlos correctamente.
#
# **Pasos:**
# 1.  **Imputaci√≥n de Nulos:** Rellenar los valores faltantes en `Charges.Total` con la mediana.
# 2.  **Codificaci√≥n de la Variable Objetivo:** Convertir la columna `Churn` a formato binario (0/1).
# 3.  **Codificaci√≥n de Variables Categ√≥ricas:** Usar One-Hot Encoding (`get_dummies`) para transformar las variables categ√≥ricas en num√©ricas.
# 4.  **Escalado de Variables Num√©ricas:** Estandarizar las variables num√©ricas para que tengan media 0 y desviaci√≥n est√°ndar 1.

# %%
# 1. Imputaci√≥n de Nulos
# Usamos la mediana porque es m√°s robusta a valores at√≠picos que la media.
median_total_charges = df_flat['Charges.Total'].median()
df_flat['Charges.Total'].fillna(median_total_charges, inplace=True)
print(f"Valores nulos restantes: {df_flat.isnull().sum().sum()}")

# 2. Codificaci√≥n de la Variable Objetivo (Target)
df_flat['Churn'] = df_flat['Churn'].map({'No': 0, 'Yes': 1})

# 3. Codificaci√≥n de Variables Categ√≥ricas
# Seleccionamos las columnas a procesar, excluyendo el ID de cliente
df_processed = df_flat.drop('customerID', axis=1)
categorical_cols = df_processed.select_dtypes(include=['object']).columns
df_processed = pd.get_dummies(df_processed, columns=categorical_cols, drop_first=True)

print("Dimensiones tras One-Hot Encoding:", df_processed.shape)

# %%
# 4. Separar variables (X) y objetivo (y), y luego escalar
X = df_processed.drop('Churn', axis=1)
y = df_processed['Churn']

# Identificar columnas num√©ricas para escalar
numerical_cols = ['tenure', 'Charges.Monthly', 'Charges.Total']

# Aplicar StandardScaler
scaler = StandardScaler()
X[numerical_cols] = scaler.fit_transform(X[numerical_cols])

print("\nPrimeras filas de los datos escalados:")
X.head()


# %% [markdown]
# ## 4Ô∏è‚É£ An√°lisis de Correlaci√≥n
# Visualizamos la correlaci√≥n de las variables con el `Churn` para identificar relaciones lineales importantes.

# %%
plt.figure(figsize=(12, 10))
# Creamos una matriz de correlaci√≥n solo con la columna 'Churn'
corr_matrix = df_processed.corr()[['Churn']].sort_values(by='Churn', ascending=False)
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlaci√≥n de Variables con Churn')
plt.show()

# %% [markdown]
# ## 5Ô∏è‚É£ Divisi√≥n de Datos y Entrenamiento de Modelos
# Dividimos los datos en conjuntos de entrenamiento (80%) y prueba (20%). Luego, entrenaremos tres modelos de clasificaci√≥n:
# 1.  **Regresi√≥n Log√≠stica:** Un modelo lineal simple y f√°cil de interpretar, ideal como l√≠nea de base.
# 2.  **Random Forest:** Un modelo de ensamble basado en √°rboles de decisi√≥n, muy potente y robusto.
# 3.  **Gradient Boosting:** Otro modelo de ensamble que suele ofrecer un rendimiento superior.

# %%
# Divisi√≥n en datos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

print(f"Tama√±o de entrenamiento: {X_train.shape[0]} muestras")
print(f"Tama√±o de prueba: {X_test.shape[0]} muestras")

# %%
# Diccionario para almacenar los modelos y sus resultados
models = {
    "Regresi√≥n Log√≠stica": LogisticRegression(random_state=42, max_iter=1000),
    "Random Forest": RandomForestClassifier(random_state=42),
    "Gradient Boosting": GradientBoostingClassifier(random_state=42)
}

results = {}

# Bucle para entrenar y evaluar cada modelo
for name, model in models.items():
    print(f"--- Entrenando {name} ---")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]
    
    # Almacenar m√©tricas
    results[name] = {
        "Accuracy": accuracy_score(y_test, y_pred),
        "ROC AUC": roc_auc_score(y_test, y_proba),
        "Classification Report": classification_report(y_test, y_pred),
        "Confusion Matrix": confusion_matrix(y_test, y_pred)
    }
    
    print(f"Evaluaci√≥n de {name}:")
    print(f"Accuracy: {results[name]['Accuracy']:.4f}")
    print(f"ROC AUC: {results[name]['ROC AUC']:.4f}")
    print(results[name]['Classification Report'])
    print("-" * 30 + "\n")

# %% [markdown]
# ## 6Ô∏è‚É£ Evaluaci√≥n y Comparaci√≥n de Modelos
# Comparamos el rendimiento de los modelos utilizando m√©tricas clave. El **ROC AUC** es especialmente √∫til en problemas con clases desbalanceadas como el churn.

# %%
# Resumen de resultados
summary_df = pd.DataFrame({
    'Modelo': list(results.keys()),
    'Accuracy': [res['Accuracy'] for res in results.values()],
    'ROC AUC': [res['ROC AUC'] for res in results.values()]
}).set_index('Modelo')

print("Resumen de Rendimiento:")
print(summary_df.sort_values(by='ROC AUC', ascending=False))

# Visualizaci√≥n de la Matriz de Confusi√≥n para el mejor modelo (basado en ROC AUC)
best_model_name = summary_df['ROC AUC'].idxmax()
best_model_cm = results[best_model_name]['Confusion Matrix']

plt.figure(figsize=(6, 5))
sns.heatmap(best_model_cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['No Churn', 'Churn'], yticklabels=['No Churn', 'Churn'])
plt.title(f'Matriz de Confusi√≥n - {best_model_name}')
plt.xlabel('Predicci√≥n')
plt.ylabel('Real')
plt.show()

# %% [markdown]
# ## 7Ô∏è‚É£ Importancia de las Variables (Feature Importance)
# Identificamos qu√© factores tienen m√°s peso en la predicci√≥n del modelo m√°s potente (Random Forest o Gradient Boosting). Esto nos da insights accionables para el negocio.

# %%
# Usar el modelo de Gradient Boosting para la importancia de las variables
best_model = models['Gradient Boosting']
importances = best_model.feature_importances_
feature_names = X.columns

# Crear un DataFrame para visualizar mejor
feature_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
}).sort_values(by='Importance', ascending=False).head(15) # Top 15

# Visualizaci√≥n
plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=feature_importance_df, palette='viridis')
plt.title('Top 15 Variables m√°s Importantes para Predecir Churn (Gradient Boosting)')
plt.xlabel('Importancia')
plt.ylabel('Variable')
plt.show()

# %% [markdown]
# ## 8Ô∏è‚É£ Conclusi√≥n Estrat√©gica
#
# ### Resumen del Rendimiento del Modelo
# El modelo **Gradient Boosting** demostr√≥ ser el m√°s eficaz, con un **ROC AUC de 0.8444**. Aunque la precisi√≥n general (Accuracy) es similar entre los modelos, su capacidad para discriminar entre clientes que cancelan y los que no es superior. La matriz de confusi√≥n revela que, si bien a√∫n hay falsos negativos (clientes que cancelan y no fueron detectados), el modelo tiene un buen balance general.
#
# ### Principales Factores que Influyen en la Cancelaci√≥n
# El an√°lisis de importancia de variables del modelo Gradient Boosting nos revela los siguientes factores clave, en orden de relevancia:
#
# 1.  **Tipo de Contrato (`Contract_Month-to-month`):** Es, con diferencia, el predictor m√°s fuerte. Los clientes sin un contrato a largo plazo tienen una alt√≠sima probabilidad de cancelar.
# 2.  **Permanencia (`tenure`):** Los clientes m√°s nuevos (baja permanencia) son mucho m√°s propensos a irse. La lealtad se construye con el tiempo.
# 3.  **Servicio de Internet (`InternetService_Fiber optic`):** Los clientes con fibra √≥ptica tienen una mayor tasa de cancelaci√≥n. Esto podr√≠a ser contraintuitivo y sugiere problemas de calidad, precio o expectativas no cumplidas con este servicio espec√≠fico.
# 4.  **Cargos Mensuales (`Charges.Monthly`):** Cargos mensuales m√°s altos est√°n correlacionados con una mayor probabilidad de cancelaci√≥n.
# 5.  **Soporte T√©cnico (`TechSupport_No`):** La falta de un servicio de soporte t√©cnico contratado es un indicador de riesgo significativo.
#
# ### üí° Recomendaciones para Telecom X
#
# - **Acci√≥n Inmediata - Campa√±as de Retenci√≥n Focalizadas:**
#   - **Objetivo:** Clientes con **contrato mensual** y **baja permanencia** (menos de 6 meses).
#   - **Oferta:** Proponerles un descuento agresivo para migrar a un contrato de 1 o 2 a√±os. Esto ataca directamente los dos principales factores de riesgo.
#
# - **Investigaci√≥n Estrat√©gica - Servicio de Fibra √ìptica:**
#   - **Pregunta Clave:** ¬øPor qu√© los clientes de fibra √≥ptica, nuestro servicio premium, cancelan m√°s?
#   - **Acci√≥n:** Realizar encuestas de satisfacci√≥n espec√≠ficas para este segmento. Analizar si el problema es el precio, la estabilidad del servicio, o si la competencia est√° ofreciendo mejores alternativas.
#
# - **Mejora del Producto - Paquetes de Servicios:**
#   - **Observaci√≥n:** La falta de servicios de valor a√±adido como **Soporte T√©cnico** y **Seguridad Online** aumenta el riesgo de cancelaci√≥n.
#   - **Acci√≥n:** Crear paquetes de servicios que incluyan `TechSupport` y `OnlineSecurity` por un costo marginal, especialmente para clientes nuevos, para aumentar su dependencia y satisfacci√≥n con el ecosistema de Telecom X.
#
# Con este modelo, Telecom X puede pasar de una estrategia reactiva a una **proactiva**, identificando clientes en riesgo antes de que tomen la decisi√≥n de irse y ofreci√©ndoles soluciones personalizadas.